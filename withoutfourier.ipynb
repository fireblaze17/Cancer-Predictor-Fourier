{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_preprocessed: (4024, 40)\n",
      "Number of encoded feature names: 40\n",
      "Numeric features after preprocessing: ['Age', 'Tumor Size', 'Regional Node Examined', 'Reginol Node Positive', 'Survival Months', 'Status']\n",
      "Categorical features after one-hot encoding: ['Race_Black', 'Race_Other', 'Race_White', 'Marital Status_Divorced', 'Marital Status_Married', 'Marital Status_Separated', 'Marital Status_Single ', 'Marital Status_Widowed', 'T Stage _T1', 'T Stage _T2', 'T Stage _T3', 'T Stage _T4', 'N Stage_N1', 'N Stage_N2', 'N Stage_N3', '6th Stage_IIA', '6th Stage_IIB', '6th Stage_IIIA', '6th Stage_IIIB', '6th Stage_IIIC', 'differentiate_Moderately differentiated', 'differentiate_Poorly differentiated', 'differentiate_Undifferentiated', 'differentiate_Well differentiated', 'Grade_ anaplastic; Grade IV', 'Grade_1', 'Grade_2', 'Grade_3', 'A Stage_Distant', 'A Stage_Regional', 'Estrogen Status_Negative', 'Estrogen Status_Positive', 'Progesterone Status_Negative', 'Progesterone Status_Positive']\n",
      "        Age  Tumor Size  Regional Node Examined  Reginol Node Positive  \\\n",
      "0  1.565253   -1.253661                1.190676              -0.618172   \n",
      "1 -0.443222    0.214345               -0.044095               0.164807   \n",
      "2  0.449434    1.540287               -0.044095               0.556296   \n",
      "3  0.449434   -0.590691               -1.525820              -0.618172   \n",
      "4 -0.777968    0.498475               -1.402343              -0.618172   \n",
      "\n",
      "   Survival Months    Status  Race_Black  Race_Other  Race_White  \\\n",
      "0        -0.492961 -0.425148         0.0         0.0         1.0   \n",
      "1        -0.405695 -0.425148         0.0         0.0         1.0   \n",
      "2         0.161530 -0.425148         0.0         0.0         1.0   \n",
      "3         0.554224 -0.425148         0.0         0.0         1.0   \n",
      "4        -0.929288 -0.425148         0.0         0.0         1.0   \n",
      "\n",
      "   Marital Status_Divorced  ...  Grade_ anaplastic; Grade IV  Grade_1  \\\n",
      "0                      0.0  ...                          0.0      0.0   \n",
      "1                      0.0  ...                          0.0      0.0   \n",
      "2                      1.0  ...                          0.0      0.0   \n",
      "3                      0.0  ...                          0.0      0.0   \n",
      "4                      0.0  ...                          0.0      0.0   \n",
      "\n",
      "   Grade_2  Grade_3  A Stage_Distant  A Stage_Regional  \\\n",
      "0      0.0      1.0              0.0               1.0   \n",
      "1      1.0      0.0              0.0               1.0   \n",
      "2      1.0      0.0              0.0               1.0   \n",
      "3      0.0      1.0              0.0               1.0   \n",
      "4      0.0      1.0              0.0               1.0   \n",
      "\n",
      "   Estrogen Status_Negative  Estrogen Status_Positive  \\\n",
      "0                       0.0                       1.0   \n",
      "1                       0.0                       1.0   \n",
      "2                       0.0                       1.0   \n",
      "3                       0.0                       1.0   \n",
      "4                       0.0                       1.0   \n",
      "\n",
      "   Progesterone Status_Negative  Progesterone Status_Positive  \n",
      "0                           0.0                           1.0  \n",
      "1                           0.0                           1.0  \n",
      "2                           0.0                           1.0  \n",
      "3                           0.0                           1.0  \n",
      "4                           0.0                           1.0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "        Age  Tumor Size  Regional Node Examined  Reginol Node Positive  \\\n",
      "0  1.565253   -1.253661                1.190676              -0.618172   \n",
      "1 -0.443222    0.214345               -0.044095               0.164807   \n",
      "2  0.449434    1.540287               -0.044095               0.556296   \n",
      "3  0.449434   -0.590691               -1.525820              -0.618172   \n",
      "4 -0.777968    0.498475               -1.402343              -0.618172   \n",
      "\n",
      "   Survival Months    Status  Race_Black  Race_Other  Race_White  \\\n",
      "0        -0.492961 -0.425148         0.0         0.0         1.0   \n",
      "1        -0.405695 -0.425148         0.0         0.0         1.0   \n",
      "2         0.161530 -0.425148         0.0         0.0         1.0   \n",
      "3         0.554224 -0.425148         0.0         0.0         1.0   \n",
      "4        -0.929288 -0.425148         0.0         0.0         1.0   \n",
      "\n",
      "   Marital Status_Divorced  ...  Grade_ anaplastic; Grade IV  Grade_1  \\\n",
      "0                      0.0  ...                          0.0      0.0   \n",
      "1                      0.0  ...                          0.0      0.0   \n",
      "2                      1.0  ...                          0.0      0.0   \n",
      "3                      0.0  ...                          0.0      0.0   \n",
      "4                      0.0  ...                          0.0      0.0   \n",
      "\n",
      "   Grade_2  Grade_3  A Stage_Distant  A Stage_Regional  \\\n",
      "0      0.0      1.0              0.0               1.0   \n",
      "1      1.0      0.0              0.0               1.0   \n",
      "2      1.0      0.0              0.0               1.0   \n",
      "3      0.0      1.0              0.0               1.0   \n",
      "4      0.0      1.0              0.0               1.0   \n",
      "\n",
      "   Estrogen Status_Negative  Estrogen Status_Positive  \\\n",
      "0                       0.0                       1.0   \n",
      "1                       0.0                       1.0   \n",
      "2                       0.0                       1.0   \n",
      "3                       0.0                       1.0   \n",
      "4                       0.0                       1.0   \n",
      "\n",
      "   Progesterone Status_Negative  Progesterone Status_Positive  \n",
      "0                           0.0                           1.0  \n",
      "1                           0.0                           1.0  \n",
      "2                           0.0                           1.0  \n",
      "3                           0.0                           1.0  \n",
      "4                           0.0                           1.0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "data = pd.read_csv('Breast_Cancer.csv')  \n",
    "\n",
    "# Divide features into numeric and categorical ones\n",
    "numeric_features = list(data.select_dtypes(include=['int64']).columns)\n",
    "categorical_features = [item for item in data.columns if item not in numeric_features]\n",
    "\n",
    "# Encode categorical features and handle missing values\n",
    "data_encoded = data.copy()\n",
    "\n",
    "\n",
    "data_encoded['Status'] = data_encoded['Status'].astype(pd.CategoricalDtype(categories=['Alive', 'Dead'], ordered=True))\n",
    "data_encoded['Status'] = data_encoded['Status'].cat.codes\n",
    "\n",
    "# Update feature types after encoding\n",
    "numeric_features = list(data_encoded.select_dtypes(include=['int8', 'int64']).columns)\n",
    "categorical_features = [item for item in data_encoded.columns if item not in numeric_features]\n",
    "\n",
    "# Create transformers for data preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Processing of missing values\n",
    "    ('scaler', StandardScaler())  # Scaling of numerical features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Processing of missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Converting categorical variables to binary numeric ones\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply transformers to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data_encoded)\n",
    "\n",
    "# encoded categorical feature names after one-hot encoding\n",
    "encoded_categorical_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(input_features=categorical_features)\n",
    "\n",
    "# Combine numeric and categorical feature names\n",
    "encoded_feature_names = numeric_features + list(encoded_categorical_feature_names)\n",
    "\n",
    "# Verify the shapes match\n",
    "print(f\"Shape of data_preprocessed: {data_preprocessed.shape}\")\n",
    "print(f\"Number of encoded feature names: {len(encoded_feature_names)}\")\n",
    "\n",
    "# Check details of generated features\n",
    "print(f\"Numeric features after preprocessing: {numeric_features}\")\n",
    "print(f\"Categorical features after one-hot encoding: {list(encoded_categorical_feature_names)}\")\n",
    "\n",
    "# Create the DataFrame\n",
    "if data_preprocessed.shape[1] == len(encoded_feature_names):\n",
    "    data_preprocessed_df = pd.DataFrame(data_preprocessed, columns=encoded_feature_names)\n",
    "    print(data_preprocessed_df.head())\n",
    "else:\n",
    "    print(f\"Mismatch! Data shape: {data_preprocessed.shape}, Feature names length: {len(encoded_feature_names)}\")\n",
    "\n",
    "# Creating a DataFrame from encoded data\n",
    "data_preprocessed_df = pd.DataFrame(data_preprocessed, columns=encoded_feature_names)\n",
    "print(data_preprocessed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Target Variable - Survival Months\n",
      "\n",
      "Training with 20.0% of the data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 20.0%):  19%|█▊        | 13/70 [00:00<00:01, 29.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/70], Loss: 0.9303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 20.0%):  36%|███▌      | 25/70 [00:00<00:01, 31.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/70], Loss: 0.7792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 20.0%):  47%|████▋     | 33/70 [00:01<00:01, 33.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/70], Loss: 0.6924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 20.0%):  64%|██████▍   | 45/70 [00:01<00:00, 33.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/70], Loss: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 20.0%):  81%|████████▏ | 57/70 [00:01<00:00, 34.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/70], Loss: 0.6565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 20.0%):  93%|█████████▎| 65/70 [00:02<00:00, 34.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/70], Loss: 0.6441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 20.0%): 100%|██████████| 70/70 [00:02<00:00, 31.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/70], Loss: 0.6336\n",
      "Test Loss: 0.7487\n",
      "MSE: 0.7487, RMSE: 0.8653, MAE: 0.7080\n",
      "\n",
      "Training with 40.0% of the data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 40.0%):  17%|█▋        | 12/70 [00:00<00:03, 16.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/70], Loss: 0.9034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 40.0%):  31%|███▏      | 22/70 [00:01<00:02, 17.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/70], Loss: 0.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 40.0%):  46%|████▌     | 32/70 [00:01<00:02, 17.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/70], Loss: 0.7602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 40.0%):  60%|██████    | 42/70 [00:02<00:01, 17.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/70], Loss: 0.7518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 40.0%):  74%|███████▍  | 52/70 [00:03<00:01, 17.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/70], Loss: 0.7422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 40.0%):  89%|████████▊ | 62/70 [00:03<00:00, 17.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/70], Loss: 0.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 40.0%): 100%|██████████| 70/70 [00:04<00:00, 17.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/70], Loss: 0.7243\n",
      "Test Loss: 0.7339\n",
      "MSE: 0.7339, RMSE: 0.8567, MAE: 0.7036\n",
      "\n",
      "Training with 60.0% of the data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 60.0%):  16%|█▌        | 11/70 [00:00<00:05, 11.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/70], Loss: 0.8244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 60.0%):  30%|███       | 21/70 [00:01<00:04, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/70], Loss: 0.7744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 60.0%):  44%|████▍     | 31/70 [00:02<00:03, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/70], Loss: 0.7644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 60.0%):  59%|█████▊    | 41/70 [00:03<00:02, 11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/70], Loss: 0.7556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 60.0%):  73%|███████▎  | 51/70 [00:04<00:01, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/70], Loss: 0.7469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 60.0%):  87%|████████▋ | 61/70 [00:05<00:00, 11.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/70], Loss: 0.7377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 60.0%): 100%|██████████| 70/70 [00:06<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/70], Loss: 0.7286\n",
      "Test Loss: 0.7365\n",
      "MSE: 0.7365, RMSE: 0.8582, MAE: 0.7085\n",
      "\n",
      "Training with 80.0% of the data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 80.0%):  16%|█▌        | 11/70 [00:01<00:07,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/70], Loss: 0.7847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 80.0%):  30%|███       | 21/70 [00:02<00:05,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/70], Loss: 0.7686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 80.0%):  44%|████▍     | 31/70 [00:03<00:04,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/70], Loss: 0.7583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 80.0%):  59%|█████▊    | 41/70 [00:04<00:03,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/70], Loss: 0.7503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 80.0%):  73%|███████▎  | 51/70 [00:06<00:02,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/70], Loss: 0.7392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 80.0%):  87%|████████▋ | 61/70 [00:07<00:01,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/70], Loss: 0.7297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 80.0%): 100%|██████████| 70/70 [00:08<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/70], Loss: 0.7187\n",
      "Test Loss: 0.7444\n",
      "MSE: 0.7444, RMSE: 0.8628, MAE: 0.7088\n",
      "\n",
      "Training with 99.9668655% of the data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 99.9668655%):  16%|█▌        | 11/70 [00:01<00:08,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/70], Loss: 0.7753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 99.9668655%):  30%|███       | 21/70 [00:03<00:07,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/70], Loss: 0.7621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 99.9668655%):  44%|████▍     | 31/70 [00:04<00:05,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/70], Loss: 0.7535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 99.9668655%):  59%|█████▊    | 41/70 [00:06<00:04,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/70], Loss: 0.7467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 99.9668655%):  73%|███████▎  | 51/70 [00:07<00:02,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/70], Loss: 0.7390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 99.9668655%):  87%|████████▋ | 61/70 [00:09<00:01,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/70], Loss: 0.7309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs (Train Size: 99.9668655%): 100%|██████████| 70/70 [00:10<00:00,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/70], Loss: 0.7227\n",
      "Test Loss: 0.7436\n",
      "MSE: 0.7436, RMSE: 0.8623, MAE: 0.7058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Custom neural network with Fourier feature generation in the forward pass\n",
    "class SurvivalPredictionNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3):\n",
    "        super(SurvivalPredictionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Define the function to train the model with different subsets of data\n",
    "def RegressionNN(feature, data, train_sizes, batch_size=32, hidden_size1=64, hidden_size2=32, hidden_size3=16, lr=0.0001, num_epochs=30):\n",
    "    print('Target Variable -', feature)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Separate the target column (Survival Months) from the rest of the data\n",
    "    X = data.drop(columns=feature)\n",
    "    y = data[feature]\n",
    "\n",
    "    # Initial split into 75% training data and 25% test data\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # From the remaining 75%, allocate 10% of the original dataset to validation (which is 13.33% of the 75%)\n",
    "    X_train_full, X_val, y_train_full, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1333, random_state=42)\n",
    "\n",
    "    # Convert test and validation data to PyTorch tensors and move to device\n",
    "    X_test = torch.tensor(X_test.values.astype(np.float32)).to(device)\n",
    "    y_test = torch.tensor(y_test.values.astype(np.float32)).reshape(-1, 1).to(device)\n",
    "    X_val = torch.tensor(X_val.values.astype(np.float32)).to(device)\n",
    "    y_val = torch.tensor(y_val.values.astype(np.float32)).reshape(-1, 1).to(device)\n",
    "\n",
    "    results = []\n",
    "    for train_size in train_sizes:\n",
    "        print(f'\\nTraining with {train_size*100}% of the data:')\n",
    "        current_train_size = int(len(X_train_full) * train_size)\n",
    "\n",
    "        # Split the training data according to the current size\n",
    "        X_train, _, y_train, _ = train_test_split(X_train_full, y_train_full, train_size=current_train_size, random_state=42)\n",
    "\n",
    "        # Convert training data to PyTorch tensors and move to device\n",
    "        X_train = torch.tensor(X_train.values.astype(np.float32)).to(device)\n",
    "        y_train = torch.tensor(y_train.values.astype(np.float32)).reshape(-1, 1).to(device)\n",
    "\n",
    "        # Creating Data Loaders for training, validation, and test datasets\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        # Initializing the model and the optimizer\n",
    "        input_size = X_train.shape[1]\n",
    "        model = SurvivalPredictionNN(input_size, hidden_size1, hidden_size2, hidden_size3).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Training with validation\n",
    "        for epoch in tqdm(range(num_epochs), desc=f\"Epochs (Train Size: {train_size*100}%)\"):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss += criterion(val_outputs, val_labels).item() * val_inputs.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Evaluate on test data after training\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                predictions.extend(outputs.squeeze(1).cpu().tolist())\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "        predictions_np = np.array(predictions)\n",
    "        y_test_np = y_test.cpu().numpy()\n",
    "        mse = mean_squared_error(y_test_np, predictions_np)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test_np, predictions_np)\n",
    "        \n",
    "        print(f'MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}')\n",
    "        \n",
    "        # Collect results\n",
    "        results.append((train_size, mse, rmse, mae))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Define train sizes and target feature\n",
    "train_sizes = [0.2, 0.4, 0.6, 0.8, 0.999668655]  # Proportions of the total data\n",
    "feature = 'Survival Months'  # Target column\n",
    "\n",
    "# Load your data here\n",
    "data = data_preprocessed_df  # Replace with your data source\n",
    "\n",
    "# Call the function\n",
    "results = RegressionNN(feature, data, train_sizes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breastcancer1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
